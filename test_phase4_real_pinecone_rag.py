"""
PHASE 4: GerÃ§ek Pinecone ile RAG Test
Mock yerine gerÃ§ek Pinecone kullanÄ±yoruz!
"""
import os
from dotenv import load_dotenv
from typing import TypedDict, List
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langgraph.graph import StateGraph, START, END

load_dotenv()

print("=" * 60)
print("ğŸ§ª PHASE 4: GerÃ§ek Pinecone ile RAG Testi")
print("=" * 60)

# Config
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "hugip-doc-index")
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-4o-mini")

# State
class GraphState(TypedDict):
    question: str
    documents: List[Document]
    generation: str

# GerÃ§ek Pinecone Vectorstore
print("\n1ï¸âƒ£ Pinecone'a baÄŸlanÄ±lÄ±yor...")
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",
    openai_api_key=os.getenv("OPENAI_API_KEY")
)

vectorstore = PineconeVectorStore(
    index_name=PINECONE_INDEX_NAME,
    embedding=embeddings
)
print(f"   âœ… Pinecone baÄŸlantÄ±sÄ± baÅŸarÄ±lÄ±! (Index: {PINECONE_INDEX_NAME})")

# LLM
llm = ChatOpenAI(model=LLM_MODEL, temperature=0.0)

# RAG Node
def retrieve_node(state: GraphState) -> GraphState:
    """GerÃ§ek Pinecone'dan dÃ¶kÃ¼man getir"""
    print("\n   ğŸ“š RAG Node: Retrieving from Pinecone...")
    
    question = state["question"]
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    documents = retriever.invoke(question)
    
    print(f"   âœ… {len(documents)} dÃ¶kÃ¼man bulundu:")
    for i, doc in enumerate(documents, 1):
        source = doc.metadata.get('source', 'Unknown')
        # Windows path'i daha okunaklÄ± yap
        source = source.split('\\')[-1] if '\\' in source else source
        print(f"      {i}. {source}")
    
    return {
        **state,
        "documents": documents
    }

# Generation Node
def generate_node(state: GraphState) -> GraphState:
    """RAG ile cevap Ã¼ret"""
    print("\n   âœ¨ Generation Node: Creating answer...")
    
    # Context hazÄ±rla
    context = "\n\n".join([doc.page_content for doc in state["documents"]])
    
    # Prompt
    prompt = ChatPromptTemplate.from_messages([
        ("system", """Sen HaliÃ§ Ãœniversitesi GiriÅŸimcilik ve Pazarlama KulÃ¼bÃ¼ asistanÄ±sÄ±n.

Verilen CONTEXT bilgilerini kullanarak soruyu TÃ¼rkÃ§e olarak yanÄ±tla.

KURALLAR:
- Context'teki bilgilere sadÄ±k kal
- EÄŸer context'te cevap yoksa, "Bu konuda dÃ¶kÃ¼manlarÄ±mda bilgi bulamadÄ±m" de
- KÄ±sa ve Ã¶z cevaplar ver
- Samimi ve yardÄ±msever ol

CONTEXT:
{context}
"""),
        ("human", "{question}")
    ])
    
    chain = prompt | llm
    response = chain.invoke({
        "context": context,
        "question": state["question"]
    })
    
    print(f"   âœ… Cevap Ã¼retildi ({len(response.content)} karakter)")
    
    return {
        **state,
        "generation": response.content
    }

# Graph oluÅŸtur
print("\n2ï¸âƒ£ RAG Graph oluÅŸturuluyor...")

workflow = StateGraph(GraphState)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("generate", generate_node)

workflow.add_edge(START, "retrieve")
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)

app = workflow.compile()

print("   âœ… Graph baÅŸarÄ±yla oluÅŸturuldu!")

# Test SorularÄ±
test_questions = [
    "KulÃ¼be nasÄ±l Ã¼ye olabilirim?",
    "KulÃ¼bÃ¼n amacÄ± nedir?",
    "YÃ¶netim Kurulu nasÄ±l seÃ§ilir?",
    "KulÃ¼p hangi etkinlikler dÃ¼zenler?",
]

print("\n" + "=" * 60)
print("3ï¸âƒ£ GerÃ§ek RAG Testi BaÅŸlÄ±yor:\n")

for i, question in enumerate(test_questions, 1):
    print(f"\n{'='*60}")
    print(f"Soru {i}: '{question}'")
    print("=" * 60)
    
    # Initial state
    initial_state = {
        "question": question,
        "documents": [],
        "generation": ""
    }
    
    # Run graph
    result = app.invoke(initial_state)
    
    print("\nğŸ“ CEVAP:")
    print("-" * 60)
    print(result['generation'])
    print("-" * 60)
    
    print(f"\nğŸ“š Kaynak dÃ¶kÃ¼manlar:")
    for j, doc in enumerate(result['documents'], 1):
        source = doc.metadata.get('source', 'Unknown')
        source = source.split('\\')[-1] if '\\' in source else source
        print(f"   {j}. {source}")

print("\n" + "=" * 60)
print("âœ… PHASE 4 TAMAMLANDI!")
print("=" * 60)
print("\nğŸ’¡ BaÅŸardÄ±klarÄ±n:")
print("   âœ… GerÃ§ek Pinecone ile RAG Ã§alÄ±ÅŸÄ±yor!")
print("   âœ… PDF'lerden bilgi getiriliyor")
print("   âœ… LLM context'e gÃ¶re cevap Ã¼retiyor")
print("   âœ… LangSmith'te her ÅŸey trace'leniyor")
print("\nğŸ”— LangSmith'te trace'leri incele:")
print("   - Her sorunun retrieval sonuÃ§larÄ±nÄ± gÃ¶r")
print("   - Generation prompt'unu incele")
print("   - Context'in nasÄ±l kullanÄ±ldÄ±ÄŸÄ±nÄ± analiz et")
print(f"   https://smith.langchain.com/o/YOUR_ORG/projects/p/{os.getenv('LANGCHAIN_PROJECT', 'club-assistant-dev')}")